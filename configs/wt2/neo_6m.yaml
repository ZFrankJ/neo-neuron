# Params (approx): total 9.7M; recurrent 6.3M; embed+proj 3.5M.
model_name: neo
dataset_name: wikitext
dataset_config: wikitext-2-raw-v1
train_split: train
val_split: validation
test_split: test

d_model: 1024
d_embed: 64
n_layers: 3
dropout: 0.1
tie_embeddings: true
cell_type: mode_c
alpha_init: 1e-1
alpha_trainable: true
alpha_min: 1e-2
alpha_max: 1e0
alpha_lr_frac: 0.1
use_checkpoint: false
use_compile: false
vocab_size: 50257

block_size: 128
batch_size: 16
epochs: 24
lr: 2e-4
weight_decay: 1e-2
weight_decay_policy: table
transformer_weight_decay: 1e-2
proj_weight_decay: 1e-3
recurrent_weight_decay: 0.0
embed_weight_decay: 0.0
alpha_weight_decay: 0.0
grad_clip: 1.0

tbptt_len: 128
train_regime: random
stream_state: false

cosine: true
warmup_epochs: 2
min_lr: 2e-5

seed: 42
save_dir: checkpoints
run_tag: wt2_neo_6m
save_each_epoch: true
resume_path: ""
restart_after_epoch: false
restart_done: false
